{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "yoochoose.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PS_QgrH2qt2s"
      },
      "source": [
        "# Session-Based Recommendation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYRRGlFgFolr"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zzhT8Z-q1i2"
      },
      "source": [
        "[YOOCHOOSE](https://www.yoochoose.com/) - [E-Commerce dataset](https://webcache.googleusercontent.com/search?q=cache:Npc6PQ3mzngJ:https://recsys.acm.org/recsys15/challenge/+&cd=1&hl=en&ct=clnk&gl=kr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExvKSbjzF0Li"
      },
      "source": [
        "## Step 1. 데이터의 전처리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_Jfq2lVGD3g"
      },
      "source": [
        "# 데이터 설명(README)를 읽어 봅니다. \n",
        "import os\n",
        "f = open('/content/drive/MyDrive/AIFFEL/Exploration/yoochoose/data/dataset-README.txt', 'r')\n",
        "while True:\n",
        "    line = f.readline()\n",
        "    if not line: break\n",
        "    print(line)\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9Jlu2aFGG5m"
      },
      "source": [
        "#################\n",
        "### Data Load ###\n",
        "#################\n",
        "import datetime as dt\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "data_path = Path('/content/drive/MyDrive/AIFFEL/Exploration/yoochoose/data/') \n",
        "train_path = data_path / 'ratings.dat'\n",
        "\n",
        "def load_data(data_path: Path, nrows=None):\n",
        "    data = pd.read_csv(data_path, sep='::', header=None, usecols=[0, 1, 2, 3], dtype={0: np.int32, 1: np.int32, 2: np.int32}, nrows=nrows)\n",
        "    data.columns = ['UserId', 'ItemId', 'Rating', 'Time']\n",
        "    return data\n",
        "\n",
        "data = load_data(train_path, None)\n",
        "data.sort_values(['UserId', 'Time'], inplace=True)  # data를 id와 시간 순서로 정렬해줍니다.\n",
        "data\n",
        "print(data)\n",
        "\n",
        "# 추천시스템을 구축할 때 가장 먼저 확인해 볼 것은 유저수(세션 수)와 아이템 수 입니다.\n",
        "\n",
        "data['UserId'].nunique(), data['ItemId'].nunique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfz9rWYVQQe8"
      },
      "source": [
        "df = data.groupby(['UserId', 'Time'])['ItemId'].count().reset_index()\n",
        "df.reset_index(inplace=True)\n",
        "df\n",
        "\n",
        "data = pd.merge(data, df, on=['UserId', 'Time']) # ItemId_x, ItemId_y, and new index\n",
        "data.drop(columns='ItemId_y', inplace=True)\n",
        "data.columns = ['UserId', 'ItemId', 'Rating', 'Time', 'SessionId'] # index to SessionId\n",
        "data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQrLN71HGXmN"
      },
      "source": [
        "######################\n",
        "### Session Length ###\n",
        "######################\n",
        "session_length = data.groupby('SessionId').size()\n",
        "print(session_length)\n",
        "\n",
        "print('-----------------------------------------------------------------------')\n",
        "\n",
        "print('len_median:', session_length.median(), 'len_mean:', session_length.mean())\n",
        "\n",
        "print('-----------------------------------------------------------------------')\n",
        "\n",
        "print('len_min:', session_length.min(), 'len_max:', session_length.max())\n",
        "\n",
        "print('-----------------------------------------------------------------------')\n",
        "\n",
        "print(session_length.quantile(0.999))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9e4AtNeU5DY"
      },
      "source": [
        "long_session = session_length[session_length==session_length.max()].index[0]\n",
        "data[data['SessionId']==long_session]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjLMEfkjGtCq"
      },
      "source": [
        "length_count = session_length.groupby(session_length).size()\n",
        "length_percent_cumsum = length_count.cumsum() / length_count.sum()\n",
        "length_percent_cumsum_999 = length_percent_cumsum[length_percent_cumsum < 0.999]\n",
        "\n",
        "length_percent_cumsum_999"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KI6ouMOUGugf"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(20, 10))\n",
        "plt.bar(x=length_percent_cumsum_999.index,\n",
        "        height=length_percent_cumsum_999, color='red')\n",
        "plt.xticks(length_percent_cumsum_999.index)\n",
        "plt.yticks(np.arange(0, 1.01, 0.05))\n",
        "plt.title('Cumsum Percentage Until 0.999', size=20)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81W-9HAGGzCe"
      },
      "source": [
        "####################\n",
        "### Session Time ###\n",
        "####################\n",
        "oldest, latest = data['Time'].min(), data['Time'].max()\n",
        "print(oldest) \n",
        "print(latest)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RD-Od-WgG-IO"
      },
      "source": [
        "######################\n",
        "### Data Cleansing ###\n",
        "######################\n",
        "# short_session을 제거한 다음 unpopular item을 제거하면 다시 길이가 1인 session이 생길 수 있습니다.\n",
        "# 이를 위해 반복문을 통해 지속적으로 제거 합니다.\n",
        "def cleanse_recursive(data: pd.DataFrame, shortest, least_click) -> pd.DataFrame:\n",
        "    while True:\n",
        "        before_len = len(data)\n",
        "        data = cleanse_short_session(data, shortest)\n",
        "        data = cleanse_unpopular_item(data, least_click)\n",
        "        after_len = len(data)\n",
        "        if before_len == after_len:\n",
        "            break\n",
        "    return data\n",
        "\n",
        "\n",
        "def cleanse_short_session(data: pd.DataFrame, shortest):\n",
        "    session_len = data.groupby('SessionId').size()\n",
        "    session_use = session_len[session_len >= shortest].index\n",
        "    data = data[data['SessionId'].isin(session_use)]\n",
        "    return data\n",
        "\n",
        "\n",
        "def cleanse_unpopular_item(data: pd.DataFrame, least_click):\n",
        "    item_popular = data.groupby('ItemId').size()\n",
        "    item_use = item_popular[item_popular >= least_click].index\n",
        "    data = data[data['ItemId'].isin(item_use)]\n",
        "    return data\n",
        "\n",
        "\n",
        "data = cleanse_recursive(data, shortest=2, least_click=5)\n",
        "data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqXwm3bbF4gZ"
      },
      "source": [
        "## Step 2. 미니 배치의 구성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBKV7n1gJpem"
      },
      "source": [
        "day2sec = 86400 # UTC time\n",
        "def split_by_date(data: pd.DataFrame, n_days: int):\n",
        "    final_time = data['Time'].max()\n",
        "    session_last_time = data.groupby('SessionId')['Time'].max()\n",
        "    session_in_train = session_last_time[session_last_time < final_time - n_days*day2sec].index\n",
        "    session_in_test = session_last_time[session_last_time >= final_time - n_days*day2sec].index\n",
        "\n",
        "    before_date = data[data['SessionId'].isin(session_in_train)]\n",
        "    after_date = data[data['SessionId'].isin(session_in_test)]\n",
        "    after_date = after_date[after_date['ItemId'].isin(before_date['ItemId'])]\n",
        "    return before_date, after_date\n",
        "\n",
        "tr, test = split_by_date(data, n_days=111)\n",
        "tr, val = split_by_date(tr, n_days=111)\n",
        "\n",
        "# data에 대한 정보를 살펴봅니다.\n",
        "def stats_info(data: pd.DataFrame, status: str):\n",
        "    print(f'* {status} Set Stats Info\\n'\n",
        "          f'\\t Events: {len(data)}\\n'\n",
        "          f'\\t Sessions: {data[\"UserId\"].nunique()}\\n'\n",
        "          f'\\t Items: {data[\"ItemId\"].nunique()}\\n'\n",
        "          f'\\t First Time : {data[\"Time\"].min()}\\n'\n",
        "          f'\\t Last Time : {data[\"Time\"].max()}\\n')\n",
        "    \n",
        "stats_info(tr, 'train')\n",
        "stats_info(val, 'valid')\n",
        "stats_info(test, 'test')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0bIHKR7Jurf"
      },
      "source": [
        "# train set에 없는 아이템이 val, test기간에 생길 수 있으므로 train data를 기준으로 인덱싱합니다.\n",
        "id2idx = {item_id : index for index, item_id in enumerate(tr['ItemId'].unique())}\n",
        "\n",
        "def indexing(df, id2idx):\n",
        "    df['item_idx'] = df['ItemId'].map(lambda x: id2idx.get(x, -1))  # id2idx에 없는 아이템은 모르는 값(-1) 처리 해줍니다.\n",
        "    return df\n",
        "\n",
        "tr = indexing(tr, id2idx)\n",
        "val = indexing(val, id2idx)\n",
        "test = indexing(test, id2idx)\n",
        "\n",
        "save_path = data_path / 'processed'\n",
        "save_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "tr.to_pickle(save_path / 'train.pkl')\n",
        "val.to_pickle(save_path / 'valid.pkl')\n",
        "test.to_pickle(save_path / 'test.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbIhbMJxJ2eI"
      },
      "source": [
        "######################\n",
        "### SessionDataset ###\n",
        "######################\n",
        "class SessionDataset:\n",
        "    \"\"\"Credit to yhs-968/pyGRU4REC.\"\"\"\n",
        "\n",
        "    def __init__(self, data):\n",
        "        self.df = data\n",
        "        self.click_offsets = self.get_click_offsets()\n",
        "        self.session_idx = np.arange(self.df['SessionId'].nunique())  # indexing to SessionId\n",
        "\n",
        "    def get_click_offsets(self):\n",
        "        \"\"\"\n",
        "        Return the indexes of the first click of each session IDs,\n",
        "        \"\"\"\n",
        "        offsets = np.zeros(self.df['SessionId'].nunique() + 1, dtype=np.int32)\n",
        "        offsets[1:] = self.df.groupby('SessionId').size().cumsum()\n",
        "        return offsets\n",
        "\n",
        "tr_dataset = SessionDataset(tr)\n",
        "tr_dataset.df.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TopHPLJHJ_6B"
      },
      "source": [
        "print(tr_dataset.click_offsets)\n",
        "\n",
        "print('-----------------------------------------------------------------------')\n",
        "\n",
        "print(tr_dataset.session_idx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJjmN0LwKGhX"
      },
      "source": [
        "#########################\n",
        "### SessionDataLoader ###\n",
        "#########################\n",
        "class SessionDataLoader:\n",
        "    \"\"\"Credit to yhs-968/pyGRU4REC.\"\"\"\n",
        "\n",
        "    def __init__(self, dataset: SessionDataset, batch_size=50):\n",
        "        self.dataset = dataset\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def __iter__(self):\n",
        "        \"\"\" Returns the iterator for producing session-parallel training mini-batches.\n",
        "        Yields:\n",
        "            input (B,):  Item indices that will be encoded as one-hot vectors later.\n",
        "            target (B,): a Variable that stores the target item indices\n",
        "            masks: Numpy array indicating the positions of the sessions to be terminated\n",
        "        \"\"\"\n",
        "\n",
        "        start, end, mask, last_session, finished = self.initialize()  # initialize 메소드에서 확인해주세요.\n",
        "        \"\"\"\n",
        "        start : Index Where Session Start\n",
        "        end : Index Where Session End\n",
        "        mask : indicator for the sessions to be terminated\n",
        "        \"\"\"\n",
        "\n",
        "        while not finished:\n",
        "            min_len = (end - start).min() - 1  # Shortest Length Among Sessions\n",
        "            for i in range(min_len):\n",
        "                # Build inputs & targets\n",
        "                inp = self.dataset.df['item_idx'].values[start + i]\n",
        "                target = self.dataset.df['item_idx'].values[start + i + 1]\n",
        "                yield inp, target, mask\n",
        "\n",
        "            start, end, mask, last_session, finished = self.update_status(start, end, min_len, last_session, finished)\n",
        "\n",
        "    def initialize(self):\n",
        "        first_iters = np.arange(self.batch_size)    # 첫 배치에 사용할 세션 Index를 가져옵니다.\n",
        "        last_session = self.batch_size - 1    # 마지막으로 다루고 있는 세션 Index를 저장해둡니다.\n",
        "        start = self.dataset.click_offsets[self.dataset.session_idx[first_iters]]       # data 상에서 session이 시작된 위치를 가져옵니다.\n",
        "        end = self.dataset.click_offsets[self.dataset.session_idx[first_iters] + 1]  # session이 끝난 위치 바로 다음 위치를 가져옵니다.\n",
        "        mask = np.array([])   # session의 모든 아이템을 다 돌은 경우 mask에 추가해줄 것입니다.\n",
        "        finished = False         # data를 전부 돌았는지 기록하기 위한 변수입니다.\n",
        "        return start, end, mask, last_session, finished\n",
        "\n",
        "    def update_status(self, start: np.ndarray, end: np.ndarray, min_len: int, last_session: int, finished: bool):  \n",
        "        # 다음 배치 데이터를 생성하기 위해 상태를 update합니다.\n",
        "        \n",
        "        start += min_len   # __iter__에서 min_len 만큼 for문을 돌았으므로 start를 min_len 만큼 더해줍니다.\n",
        "        mask = np.arange(self.batch_size)[(end - start) == 1]  \n",
        "        # end는 다음 세션이 시작되는 위치인데 start와 한 칸 차이난다는 것은 session이 끝났다는 뜻입니다. mask에 기록해줍니다.\n",
        "\n",
        "        for i, idx in enumerate(mask, start=1):  # mask에 추가된 세션 개수만큼 새로운 세션을 돌것입니다.\n",
        "            new_session = last_session + i  \n",
        "            if new_session > self.dataset.session_idx[-1]:  # 만약 새로운 세션이 마지막 세션 index보다 크다면 모든 학습데이터를 돈 것입니다.\n",
        "                finished = True\n",
        "                break\n",
        "            # update the next starting/ending point\n",
        "            start[idx] = self.dataset.click_offsets[self.dataset.session_idx[new_session]]     # 종료된 세션 대신 새로운 세션의 시작점을 기록합니다.\n",
        "            end[idx] = self.dataset.click_offsets[self.dataset.session_idx[new_session] + 1]\n",
        "\n",
        "        last_session += len(mask)  # 마지막 세션의 위치를 기록해둡니다.\n",
        "        return start, end, mask, last_session, finished\n",
        "\n",
        "\n",
        "tr_data_loader = SessionDataLoader(tr_dataset, batch_size=4)\n",
        "tr_dataset.df.head(15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2OvZm6WyKT1q"
      },
      "source": [
        "iter_ex = iter(tr_data_loader)\n",
        "\n",
        "inputs, labels, mask =  next(iter_ex)\n",
        "print(f'Model Input Item Idx are : {inputs}')\n",
        "print(f'Label Item Idx are : {\"\":5} {labels}')\n",
        "print(f'Previous Masked Input Idx are {mask}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nT35rnoAF6pW"
      },
      "source": [
        "## Step 3. 모델 구성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Am7dfdqsKVcI"
      },
      "source": [
        "#########################\n",
        "### Evaluation Metric ###\n",
        "#########################\n",
        "def mrr_k(pred, truth: int, k: int):\n",
        "    indexing = np.where(pred[:k] == truth)[0]\n",
        "    if len(indexing) > 0:\n",
        "        return 1 / (indexing[0] + 1)\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "\n",
        "def recall_k(pred, truth: int, k: int) -> int:\n",
        "    answer = truth in pred[:k]\n",
        "    return int(answer)\n",
        "\n",
        "\n",
        "##########################\n",
        "### Model Architecture ###\n",
        "##########################\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, GRU\n",
        "from tensorflow.keras.losses import categorical_crossentropy\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tqdm import tqdm\n",
        "\n",
        "def create_model(args):\n",
        "    inputs = Input(batch_shape=(args.batch_size, 1, args.num_items))\n",
        "    gru, _ = GRU(args.hsz, stateful=True, return_state=True, name='GRU')(inputs)\n",
        "    dropout = Dropout(args.drop_rate)(gru)\n",
        "    predictions = Dense(args.num_items, activation='softmax')(dropout)\n",
        "    model = Model(inputs=inputs, outputs=[predictions])\n",
        "    model.compile(loss=categorical_crossentropy, optimizer=Adam(args.lr), metrics=['accuracy'])\n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "class Args:\n",
        "    def __init__(self, tr, val, test, batch_size, hsz, drop_rate, lr, epochs, k):\n",
        "        self.tr = tr\n",
        "        self.val = val\n",
        "        self.test = test\n",
        "        self.num_items = tr['ItemId'].nunique()\n",
        "        self.num_sessions = tr['SessionId'].nunique()\n",
        "        self.batch_size = batch_size\n",
        "        self.hsz = hsz\n",
        "        self.drop_rate = drop_rate\n",
        "        self.lr = lr\n",
        "        self.epochs = epochs\n",
        "        self.k = k\n",
        "\n",
        "args = Args(tr, val, test, batch_size=128, hsz=50, drop_rate=0.1, lr=0.001, epochs=10, k=20)\n",
        "\n",
        "model = create_model(args)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5KcFLr-F7_W"
      },
      "source": [
        "## Step 4. 모델 학습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYE3kz2_KjRU"
      },
      "source": [
        "######################\n",
        "### Model Training ###\n",
        "######################\n",
        "# train 셋으로 학습하면서 valid 셋으로 검증합니다.\n",
        "def train_model(model, args):\n",
        "    train_dataset = SessionDataset(args.tr)\n",
        "    train_loader = SessionDataLoader(train_dataset, batch_size=args.batch_size)\n",
        "\n",
        "    for epoch in range(1, args.epochs + 1):\n",
        "        total_step = len(args.tr) - args.tr['SessionId'].nunique()\n",
        "        tr_loader = tqdm(train_loader, total=total_step // args.batch_size, desc='Train', mininterval=1)\n",
        "        for feat, target, mask in tr_loader:\n",
        "            reset_hidden_states(model, mask)  # 종료된 session은 hidden_state를 초기화합니다. 아래 메서드에서 확인해주세요.\n",
        "\n",
        "            input_ohe = to_categorical(feat, num_classes=args.num_items)\n",
        "            input_ohe = np.expand_dims(input_ohe, axis=1)\n",
        "            target_ohe = to_categorical(target, num_classes=args.num_items)\n",
        "\n",
        "            result = model.train_on_batch(input_ohe, target_ohe)\n",
        "            tr_loader.set_postfix(train_loss=result[0], accuracy = result[1])\n",
        "\n",
        "        val_recall, val_mrr = get_metrics(args.val, model, args, args.k)  # valid set에 대해 검증합니다.\n",
        "\n",
        "        print(f\"\\t - Recall@{args.k} epoch {epoch}: {val_recall:3f}\")\n",
        "        print(f\"\\t - MRR@{args.k}    epoch {epoch}: {val_mrr:3f}\\n\")\n",
        "\n",
        "\n",
        "def reset_hidden_states(model, mask):\n",
        "    gru_layer = model.get_layer(name='GRU')  # model에서 gru layer를 가져옵니다.\n",
        "    hidden_states = gru_layer.states[0].numpy()  # gru_layer의 parameter를 가져옵니다.\n",
        "    for elt in mask:  # mask된 인덱스 즉, 종료된 세션의 인덱스를 돌면서\n",
        "        hidden_states[elt, :] = 0  # parameter를 초기화 합니다.\n",
        "    gru_layer.reset_states(states=hidden_states)\n",
        "\n",
        "\n",
        "def get_metrics(data, model, args, k: int):  # valid셋과 test셋을 평가하는 코드입니다. \n",
        "                                             # train과 거의 같지만 mrr, recall을 구하는 라인이 있습니다.\n",
        "    dataset = SessionDataset(data)\n",
        "    loader = SessionDataLoader(dataset, batch_size=args.batch_size)\n",
        "    recall_list, mrr_list = [], []\n",
        "\n",
        "    total_step = len(data) - data['SessionId'].nunique()\n",
        "    for inputs, label, mask in tqdm(loader, total=total_step // args.batch_size, desc='Evaluation', mininterval=1):\n",
        "        reset_hidden_states(model, mask)\n",
        "        input_ohe = to_categorical(inputs, num_classes=args.num_items)\n",
        "        input_ohe = np.expand_dims(input_ohe, axis=1)\n",
        "\n",
        "        pred = model.predict(input_ohe, batch_size=args.batch_size)\n",
        "        pred_arg = tf.argsort(pred, direction='DESCENDING')  # softmax 값이 큰 순서대로 sorting 합니다.\n",
        "\n",
        "        length = len(inputs)\n",
        "        recall_list.extend([recall_k(pred_arg[i], label[i], k) for i in range(length)])\n",
        "        mrr_list.extend([mrr_k(pred_arg[i], label[i], k) for i in range(length)])\n",
        "\n",
        "    recall, mrr = np.mean(recall_list), np.mean(mrr_list)\n",
        "    return recall, mrr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXLuG7KCKoAr"
      },
      "source": [
        "# 학습 시간이 다소 오래 소요됩니다. (예상시간 1시간)\n",
        "train_model(model, args)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "naBQYpEwF9fW"
      },
      "source": [
        "## Step 5. 모델 테스트"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yA1XNLy8KqLJ"
      },
      "source": [
        "#################\n",
        "### Inference ###\n",
        "#################\n",
        "def test_model(model, args, test):\n",
        "    test_recall, test_mrr = get_metrics(test, model, args, 20)\n",
        "    print(f\"\\t - Recall@{args.k}: {test_recall:3f}\")\n",
        "    print(f\"\\t - MRR@{args.k}: {test_mrr:3f}\\n\")\n",
        "\n",
        "test_model(model, args, test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQPI2h47os2o"
      },
      "source": [
        "결론\n",
        "- Movielens 데이터셋을 session based recommendation 관점으로 길이분석, 시간분석 등으로 전처리 과정을 진행\n",
        "- RNN 기반의 예측 모델을 설계하고 학습이 진행되는 과정에서 train loss가 안정적으로 감소하고, validation 단계에서의 Recall, MRR 개선\n",
        "- 세션정의, 모델구조, 하이퍼파라미터 등을 변경해서 실험하여 Recall, MRR 등의 변화추이를 관찰 및 분석 진행"
      ]
    }
  ]
}