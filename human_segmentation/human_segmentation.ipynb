{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "human_segmentation.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8wkft7rSbSZ"
      },
      "source": [
        "# Sementic segmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4q92rA51Si--"
      },
      "source": [
        "#################\n",
        "### Libraries ###\n",
        "#################\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import tarfile\n",
        "import urllib # 웹에서 데이터를 다운로드\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "img_rdj_path = '/content/drive/MyDrive/AIFFEL/Exploration/human_segmentation/images/rdj.jpg'\n",
        "img_pup_path = '/content/drive/MyDrive/AIFFEL/Exploration/human_segmentation/images/puppy.jpg'\n",
        "img_rdj_orig = cv2.imread(img_rdj_path) \n",
        "img_pup_orig = cv2.imread(img_pup_path) \n",
        "print(img_rdj_orig.shape)\n",
        "print(img_pup_orig.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDHoYM4OSuLw"
      },
      "source": [
        "###################################\n",
        "### DeepLab Model (tf version1) ###\n",
        "###################################\n",
        "class DeepLabModel(object):\n",
        "    INPUT_TENSOR_NAME = 'ImageTensor:0'\n",
        "    OUTPUT_TENSOR_NAME = 'SemanticPredictions:0'\n",
        "    INPUT_SIZE = 513\n",
        "    FROZEN_GRAPH_NAME = 'frozen_inference_graph'\n",
        "\n",
        "    # __init__()에서 모델 구조를 직접 구현하는 대신, tar file에서 읽어들인 그래프구조 graph_def를 \n",
        "    # tf.compat.v1.import_graph_def를 통해 불러들여 활용하게 됩니다. \n",
        "    def __init__(self, tarball_path):\n",
        "        self.graph = tf.Graph()\n",
        "        graph_def = None\n",
        "        tar_file = tarfile.open(tarball_path)\n",
        "        for tar_info in tar_file.getmembers():\n",
        "            if self.FROZEN_GRAPH_NAME in os.path.basename(tar_info.name):\n",
        "                file_handle = tar_file.extractfile(tar_info)\n",
        "                graph_def = tf.compat.v1.GraphDef.FromString(file_handle.read())\n",
        "                break\n",
        "        tar_file.close()\n",
        "\n",
        "        with self.graph.as_default():\n",
        "    \t    tf.compat.v1.import_graph_def(graph_def, name='')\n",
        "\n",
        "        self.sess = tf.compat.v1.Session(graph=self.graph)\n",
        "\n",
        "    # 이미지를 전처리하여 Tensorflow 입력으로 사용 가능한 shape의 Numpy Array로 변환합니다.\n",
        "    def preprocess(self, img_orig):\n",
        "        height, width = img_orig.shape[:2]\n",
        "        resize_ratio = 1.0 * self.INPUT_SIZE / max(width, height)\n",
        "        target_size = (int(resize_ratio * width), int(resize_ratio * height))\n",
        "        resized_image = cv2.resize(img_orig, target_size)\n",
        "        resized_rgb = cv2.cvtColor(resized_image, cv2.COLOR_BGR2RGB)\n",
        "        img_input = resized_rgb\n",
        "        return img_input\n",
        "        \n",
        "    def run(self, image):\n",
        "        img_input = self.preprocess(image)\n",
        "\n",
        "        # Tensorflow V1에서는 model(input) 방식이 아니라 sess.run(feed_dict={input...}) 방식을 활용합니다.\n",
        "        batch_seg_map = self.sess.run(\n",
        "            self.OUTPUT_TENSOR_NAME,\n",
        "            feed_dict={self.INPUT_TENSOR_NAME: [img_input]})\n",
        "\n",
        "        seg_map = batch_seg_map[0]\n",
        "        return cv2.cvtColor(img_input, cv2.COLOR_RGB2BGR), seg_map"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zagNuM0ISwxC"
      },
      "source": [
        "##########################################################\n",
        "### Define model and download & load pretrained weight ###\n",
        "##########################################################\n",
        "_DOWNLOAD_URL_PREFIX = 'http://download.tensorflow.org/models/'\n",
        "\n",
        "model_dir = '/content/drive/MyDrive/AIFFEL/Exploration/human_segmentation/models'\n",
        "tf.io.gfile.makedirs(model_dir)\n",
        "\n",
        "print('temp directory:', model_dir)\n",
        "\n",
        "download_path = os.path.join(model_dir, 'deeplab_model.tar.gz')\n",
        "if not os.path.exists(download_path):\n",
        "    urllib.request.urlretrieve(_DOWNLOAD_URL_PREFIX + 'deeplabv3_mnv2_pascal_train_aug_2018_01_29.tar.gz',\n",
        "                   download_path)\n",
        "\n",
        "MODEL = DeepLabModel(download_path)\n",
        "print('model loaded successfully!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zig195hQSyRo"
      },
      "source": [
        "###################################\n",
        "### Resize image & Check labels ###\n",
        "###################################\n",
        "img_rdj_resized, rdj_seg_map = MODEL.run(img_rdj_orig)\n",
        "rdj_label = rdj_seg_map.max()\n",
        "print('RDJ image: ', img_rdj_orig.shape, img_rdj_resized.shape, rdj_label)\n",
        "\n",
        "img_pup_resized, pup_seg_map = MODEL.run(img_pup_orig)\n",
        "pup_label = pup_seg_map.max()\n",
        "print('Puppy image: ', img_pup_orig.shape, img_pup_resized.shape, pup_label)\n",
        "\n",
        "LABEL_NAMES = [\n",
        "    'background', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus',\n",
        "    'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike',\n",
        "    'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tv'\n",
        "]\n",
        "print('Total labels:', len(LABEL_NAMES))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wguxRLAkS3O7"
      },
      "source": [
        "#######################################\n",
        "### Show semantic segmentation mask ###\n",
        "#######################################\n",
        "def showSemSegMask(img_resized, seg_map, obj_label):\n",
        "    img_show = img_resized.copy()\n",
        "    seg_map = np.where(seg_map == obj_label, obj_label, 0) # 예측 중 사람만 추출\n",
        "    img_mask = seg_map * (255/seg_map.max()) # 255 normalization\n",
        "    img_mask = img_mask.astype(np.uint8)\n",
        "    color_mask = cv2.applyColorMap(img_mask, cv2.COLORMAP_JET)\n",
        "    img_show = cv2.addWeighted(img_show, 0.6, color_mask, 0.35, 0.0)\n",
        "\n",
        "    plt.imshow(cv2.cvtColor(img_show, cv2.COLOR_BGR2RGB))\n",
        "    plt.show()\n",
        "\n",
        "    return img_mask\n",
        "\n",
        "img_rdj_mask = showSemSegMask(img_rdj_resized, rdj_seg_map, rdj_label)\n",
        "img_pup_mask = showSemSegMask(img_pup_resized, pup_seg_map, pup_label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDUe65Kr3orF"
      },
      "source": [
        " rdj_label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAEb5QrwS5mN"
      },
      "source": [
        "#####################\n",
        "### Original size ###\n",
        "#####################\n",
        "def resize2orig(img_mask, img_orig):\n",
        "    img_mask_up = cv2.resize(img_mask, img_orig.shape[:2][::-1], interpolation=cv2.INTER_LINEAR)\n",
        "    _, img_mask_up = cv2.threshold(img_mask_up, 128, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "    ax = plt.subplot(1,2,1)\n",
        "    plt.imshow(img_mask_up, cmap=plt.cm.binary_r)\n",
        "    ax.set_title('Original Size Mask')\n",
        "\n",
        "    ax = plt.subplot(1,2,2)\n",
        "    plt.imshow(img_mask, cmap=plt.cm.binary_r)\n",
        "    ax.set_title('DeepLab Model Mask')\n",
        "    plt.show()\n",
        "\n",
        "    return img_mask_up\n",
        "    \n",
        "img_rdj_mask_up = resize2orig(img_rdj_mask, img_rdj_orig)\n",
        "img_pup_mask_up = resize2orig(img_pup_mask, img_pup_orig)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2WvqQTXS9_H"
      },
      "source": [
        "###########################\n",
        "### Original blur image ###\n",
        "###########################\n",
        "def showBlur(img_orig):\n",
        "    img_orig_blur = cv2.blur(img_orig, (13,13)) #(13,13)은 blurring  kernel size를 뜻합니다. \n",
        "    plt.imshow(cv2.cvtColor(img_orig_blur, cv2.COLOR_BGR2RGB))\n",
        "    plt.show() # img_orig_blur 결과\n",
        "\n",
        "    return img_orig_blur\n",
        "\n",
        "img_rdj_orig_blur = showBlur(img_rdj_orig)\n",
        "img_pup_orig_blur = showBlur(img_pup_orig)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jADoqM6NS_Vi"
      },
      "source": [
        "#############################\n",
        "### Background blur image ###\n",
        "#############################\n",
        "def showBlurBackground(img_mask_up, img_orig_blur):\n",
        "    img_mask_color = cv2.cvtColor(img_mask_up, cv2.COLOR_GRAY2BGR)\n",
        "    img_bg_mask = cv2.bitwise_not(img_mask_color)\n",
        "    img_bg_blur = cv2.bitwise_and(img_orig_blur, img_bg_mask)\n",
        "    plt.imshow(cv2.cvtColor(img_bg_blur, cv2.COLOR_BGR2RGB))\n",
        "    plt.show()\n",
        "\n",
        "    return img_mask_color, img_bg_blur\n",
        "\n",
        "img_rdj_mask_color, img_rdj_bg_blur = showBlurBackground(img_rdj_mask_up, img_rdj_orig_blur)\n",
        "img_pup_mask_color, img_pup_bg_blur = showBlurBackground(img_pup_mask_up, img_pup_orig_blur)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vkRJnPtTBWz"
      },
      "source": [
        "########################\n",
        "### Composite image ###\n",
        "########################\n",
        "def blurComposite(img_mask_color, img_orig, img_bg_blur):\n",
        "    img_concat = np.where(img_mask_color==255, img_orig, img_bg_blur)\n",
        "    plt.imshow(cv2.cvtColor(img_concat, cv2.COLOR_BGR2RGB))\n",
        "    plt.show()\n",
        "\n",
        "    return img_concat\n",
        "\n",
        "img_rdj_concat = blurComposite(img_rdj_mask_color, img_rdj_orig, img_rdj_bg_blur)\n",
        "img_pup_concat = blurComposite(img_pup_mask_color, img_pup_orig, img_pup_bg_blur)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhS3DaAP6Qi0"
      },
      "source": [
        "fig = plt.figure(figsize=(10,10))\n",
        "\n",
        "ax1 = fig.add_subplot(2,2,1)\n",
        "ax1.imshow(cv2.cvtColor(img_rdj_orig, cv2.COLOR_BGR2RGB))\n",
        "ax1.set_title('Original RDJ image')\n",
        "\n",
        "ax2 = fig.add_subplot(2,2,2)\n",
        "ax2.imshow(cv2.cvtColor(img_rdj_concat, cv2.COLOR_BGR2RGB))\n",
        "ax2.set_title('Outfocus RDJ image')\n",
        "\n",
        "ax3 = fig.add_subplot(2,2,3)\n",
        "ax3.imshow(cv2.cvtColor(img_pup_orig, cv2.COLOR_BGR2RGB))\n",
        "ax3.set_title('Original Puppy image')\n",
        "\n",
        "ax4 = fig.add_subplot(2,2,4)\n",
        "ax4.imshow(cv2.cvtColor(img_pup_concat, cv2.COLOR_BGR2RGB))\n",
        "ax4.set_title('Outfocus Puppy image')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4zJRlBBAFW6"
      },
      "source": [
        "#################\n",
        "### Chromakey ###\n",
        "#################\n",
        "target_height = min(img_rdj_resized.shape[0], img_pup_resized.shape[0])\n",
        "target_width = min(img_rdj_resized.shape[1], img_pup_resized.shape[1])\n",
        "target_size=(target_width, target_height)\n",
        "\n",
        "img_rdj_orig_resized = cv2.resize(img_rdj_orig, target_size)\n",
        "img_rdj_bg_blur_resized = cv2.resize(img_rdj_bg_blur, target_size)\n",
        "img_rdj_mask_color_resize = cv2.resize(img_rdj_mask_color, target_size)\n",
        "\n",
        "img_pup_orig_resized = cv2.resize(img_pup_orig, target_size)\n",
        "img_pup_bg_blur_resized = cv2.resize(img_pup_bg_blur, target_size)\n",
        "img_pup_mask_color_resize = cv2.resize(img_pup_mask_color, target_size)\n",
        "\n",
        "img_rdj_pup_concat = blurComposite(img_rdj_mask_color_resize, img_rdj_orig_resized, img_pup_bg_blur_resized)\n",
        "\n",
        "img_pup_rdj_concat = blurComposite(img_pup_mask_color_resize, img_pup_orig_resized, img_rdj_bg_blur_resized)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7vWkAxsJ_Vh"
      },
      "source": [
        "from IPython.display import Image\n",
        "Image('/content/drive/MyDrive/AIFFEL/Exploration/human_segmentation/result_rdj_3.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-nGJyv6DX-C"
      },
      "source": [
        "위의 문제점을 해결하기 위한 방법\n",
        "\n",
        "- Use 3D Depth (Stereo) Camera\n",
        "    - 한 장면에 대해서 stereo image를 가지고 있다면, 우리는 그 두 장의 이미지로부터 disparity 얻을 수 있고,\n",
        "    - disparities, focal lengths, baseline을 가지고 triangulation 계산을 통하여 depth 값 계산 가능\n",
        "    - 각 픽셀에 대한 depth 정보를 알고 있다면, sementic segmentation mask 과정에서 객체와 배경을 구분 할 때 depth 값이 차이가 많이나는 지점을 경계로 판단하여 조금 더 경계를 명확히 표현 할 수 있을 것이라 생각됨\n",
        "    - f는 focal length로 이미지와 카메라 렌즈 사이 거리\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKEgoi9LV10s"
      },
      "source": [
        "Image('/content/drive/MyDrive/AIFFEL/Exploration/human_segmentation/depthDerivations.png') # Figure from D. Lowe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTTw9DGxbpja"
      },
      "source": [
        "#####################\n",
        "### Stereo images ###\n",
        "#####################\n",
        "from PIL import Image\n",
        "img_l = cv2.imread('/content/drive/MyDrive/AIFFEL/Exploration/human_segmentation/avatar_l.png')\n",
        "img_r = cv2.imread('/content/drive/MyDrive/AIFFEL/Exploration/human_segmentation/avatar_r.png')\n",
        "img_l = cv2.cvtColor(img_l, cv2.COLOR_RGB2BGR)\n",
        "img_r = cv2.cvtColor(img_r, cv2.COLOR_RGB2BGR)\n",
        "plt.imshow(img_l)\n",
        "plt.imshow(img_r, alpha=0.5)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qaFidZJJW6m6"
      },
      "source": [
        "#################\n",
        "### Disparity ###\n",
        "#################\n",
        "img_l = cv2.imread('/content/drive/MyDrive/AIFFEL/Exploration/human_segmentation/avatar_l.png',0)\n",
        "img_r = cv2.imread('/content/drive/MyDrive/AIFFEL/Exploration/human_segmentation/avatar_r.png',0)\n",
        "\n",
        "stereo = cv2.StereoBM_create(numDisparities=16, blockSize=15)\n",
        "disparity = stereo.compute(img_l,img_r)\n",
        "plt.imshow(disparity,'gray')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XytbWenpc1f4"
      },
      "source": [
        "**결론**    \n",
        "- 아웃포커싱 효과가 적용된 인물모드 사진과 강아지 사진, 그리고 두 사진의 배경을 서로 바꿔서 적용해본 크로마키사진이 결과로 나옴\n",
        "- 인물사진에서 사람의 몸과 배경의 구분이 명확하지 않는 문제점이 존재하였으며, 해당 부분을 사진에 표시함\n",
        "- 추가적인 depth 정보를 활용한다면 배경과 인물을 구분하는 경계를 조금 더 뚜렷하게 표현 가능할 것이라고 판단됨"
      ]
    }
  ]
}