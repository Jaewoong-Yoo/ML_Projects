{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "lyricist.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KaNuzE6TOcG2"
      },
      "source": [
        "# lyricist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsD7BRV1F7Oi"
      },
      "source": [
        "## Read data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9ivxmQCaZHy",
        "outputId": "36bdf7db-dde2-4755-d8b6-49b537efa358"
      },
      "source": [
        "import glob, os, re\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#################\n",
        "### Read data ###\n",
        "#################\n",
        "txt_file_path = r\"/content/drive/MyDrive/AIFFEL/Exploration/lyricist/data/lyrics/*\"\n",
        "\n",
        "txt_list = glob.glob(txt_file_path)\n",
        "\n",
        "raw_corpus = []\n",
        "\n",
        "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n",
        "for txt_file in txt_list:\n",
        "    with open(txt_file, \"r\") as f: # 읽기전용으로 file을 불러온다. file as f\n",
        "        raw = f.read().splitlines() # file을 한줄씩 읽어오는데 .splitlines()로 종료문자 \\n을 포함하지 않음.\n",
        "        raw_corpus.extend(raw)\n",
        "\n",
        "print(\"데이터 크기:\", len(raw_corpus))\n",
        "print(\"Examples:\\n\", raw_corpus[:3]) # 0~3 인덱스까지"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "데이터 크기: 187088\n",
            "Examples:\n",
            " ['Well, summer slipped us underneath her tongue', 'Our days and nights are perfumed with obsession', 'Half of my wardrobe is on your bedroom floor']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Nk6gisgF5CF"
      },
      "source": [
        "## Data cleansing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dj8oYW00z5qL",
        "outputId": "ee985cfc-1b54-4abe-ad8f-4f3210435c4e"
      },
      "source": [
        "################################################\n",
        "### Data cleansing using regular expressions ###\n",
        "################################################\n",
        "def preprocess_sentence(sentence):\n",
        "    sentence = sentence.lower().strip() # 소문자로 바꾸고(.lower()), 양쪽 공백 제거(.strip())\n",
        "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) # 특수문자 양쪽에 공백 삽입\n",
        "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 여러공백은 하나의 공백으로\n",
        "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) # a-zA-Z?.!,¿ 가 아닌 모든문자를 하나의 공백으로 바꾼다\n",
        "    sentence = sentence.strip() # 다시 양쪽공백지움\n",
        "    sentence = '<start> ' + sentence + ' <end>' # 문장 시작에는 <start>추가, 끝에는 <end>추가\n",
        "    return sentence\n",
        "\n",
        "print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))\n",
        "\n",
        "corpus = []   #빈 리스트 생성\n",
        "\n",
        "for sentence in raw_corpus:   #raw_corpus: 한 줄 단위로 저장된 배열\n",
        "    if len(sentence) == 0: continue   #한 글자도 없으면 continue\n",
        "    if len(sentence.split()) > 15: continue   # 토큰의 개수가 15개를 넘어가는 문장 제외\n",
        "    if sentence[-1] == \":\": continue   #마지막 글자가 \":\"이면 continue\n",
        "    \n",
        "    preprocessed_sentence = preprocess_sentence(sentence)   #정규식에 따라 문자열 변환\n",
        "    corpus.append(preprocessed_sentence)   #리스트에 추가하기\n",
        "        \n",
        "print(corpus[:10])   #0 ~ 9의 요소"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<start> this is sample sentence . <end>\n",
            "['<start> well , summer slipped us underneath her tongue <end>', '<start> our days and nights are perfumed with obsession <end>', '<start> half of my wardrobe is on your bedroom floor <end>', '<start> use our eyes , throw our hands overboard i am your sweetheart psychopathic crush <end>', '<start> drink up your movements , still i can t get enough <end>', '<start> i overthink your p punctuation use <end>', '<start> not my fault , just a thing that my mind do a rush at the beginning <end>', '<start> i get caught up , just for a minute <end>', '<start> but lover , you re the one to blame , all that you re doing <end>', '<start> can you hear the violence ? <end>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5XshBVYFz8C"
      },
      "source": [
        "## Tokenize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ni4_dm4j1kiO",
        "outputId": "d45698a7-c56e-4c43-8b14-4ade61116032"
      },
      "source": [
        "################\n",
        "### Tokenize ###\n",
        "################\n",
        "def tokenize(corpus):\n",
        "    # 토큰화 시 텐서플로우의 Tokenizer와 pad_sequences 사용\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "        num_words=12000, # 12000단어를 기억할 수 있는 tokenizer를 만들기 (각 단어에 인덱스를 부여)\n",
        "        filters=' ', # filter == empty, (default값: '!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\\t\\n')\n",
        "        oov_token=\"<unk>\" # 12000단어에 포함되지 못하면 <unk>로 변환\n",
        "    )\n",
        "    tokenizer.fit_on_texts(corpus) # 문자데이터를 입력받아서 리스트의 형태로 변환\n",
        "    tensor = tokenizer.texts_to_sequences(corpus) # 텍스트 안의 단어들을 숫자의 시퀀스형태(tensor)로 변환  \n",
        "\n",
        "    # 입력 데이터의 시퀀스 길이를 일정하게 맞추되, 시퀀스가 짧으면 문장 뒤에 패딩 <pad> 붙여서 길이 맞추기\n",
        "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')  # 0을 이용하여 같은 길이의 시퀀스로 변환\n",
        "    # 만약 문장 앞에 패딩을 붙여 길이를 맞추고 싶다면 padding='pre'사용\n",
        "    \n",
        "    print('tensor :', '\\n', tensor, '\\n', 'tokenizer :', '\\n', tokenizer)\n",
        "    return tensor, tokenizer\n",
        "\n",
        "tensor, tokenizer = tokenize(corpus)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor : \n",
            " [[  2 142   4 ...   0   0   0]\n",
            " [  2 153 365 ...   0   0   0]\n",
            " [  2 540  19 ...   0   0   0]\n",
            " ...\n",
            " [  2   3   0 ...   0   0   0]\n",
            " [  2   3   0 ...   0   0   0]\n",
            " [  2   3   0 ...   0   0   0]] \n",
            " tokenizer : \n",
            " <keras_preprocessing.text.Tokenizer object at 0x7f7bca0ea750>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4TwPXV4w2gfw",
        "outputId": "de2d6477-81fc-4995-b4d7-e129591acd79"
      },
      "source": [
        "print(tensor[:3, :10])\n",
        "\n",
        "for idx in tokenizer.index_word:\n",
        "    print(idx, \":\", tokenizer.index_word[idx]) # tokenizer변수에 저장되어 있는 단어 사전의 인덱스\n",
        "\n",
        "    if idx >= 10: break # word_index가 10일 때 까지 출력하기"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[   2  142    4  557 3121  126 1217   69  957    3]\n",
            " [   2  153  365    8  833   77 9158   31 9159    3]\n",
            " [   2  540   19   13 5081   26   18   21 1454  357]]\n",
            "1 : <unk>\n",
            "2 : <start>\n",
            "3 : <end>\n",
            "4 : ,\n",
            "5 : i\n",
            "6 : the\n",
            "7 : you\n",
            "8 : and\n",
            "9 : a\n",
            "10 : to\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2FCC3OzFqoO"
      },
      "source": [
        "## Data splitting - train/test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "imHYIpg42q2v",
        "outputId": "1027082d-43df-43a4-992e-2b7c18205aa3"
      },
      "source": [
        "src_input = tensor[:, :-1]  # tensor 의 모든 행에서 마지막열을 제외한 src_input생성\n",
        "tgt_input = tensor[:, 1:] # tensor의 모든 행에서 첫번째열(start)을 제외한 tgt_input생성\n",
        "\n",
        "print(src_input[0])\n",
        "print(tgt_input[0])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[   2  142    4  557 3121  126 1217   69  957    3    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0]\n",
            "[ 142    4  557 3121  126 1217   69  957    3    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PedDULhRGnjq",
        "outputId": "4fea42e0-176f-44a2-c736-1bfa43854c65"
      },
      "source": [
        "# 훈련 데이터와 평가 데이터 분리, 총 데이터의 20% 를 평가 데이터셋\n",
        "enc_train, enc_test, dec_train, dec_test = train_test_split(src_input, tgt_input, test_size=0.2, ram)\n",
        "print(f\"enc_train: {enc_train.shape}\")\n",
        "print(f\"enc_test: {enc_test.shape}\")\n",
        "print(f\"dec_train: {dec_train.shape}\")\n",
        "print(f\"dec_test: {dec_test.shape}\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "enc_train: (134685, 32)\n",
            "enc_test: (33672, 32)\n",
            "dec_train: (134685, 32)\n",
            "dec_test: (33672, 32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9oBCfvK249S",
        "outputId": "f029d388-16e0-4805-b97e-5b13ce9b7c8e"
      },
      "source": [
        "BUFFER_SIZE = len(src_input) #입력 문장 수\n",
        "BATCH_SIZE = 256\n",
        "steps_per_epoch = len(src_input) // BATCH_SIZE #입력 문장 수를 배치사이즈로 에포크 시행마다 나누어 훈련\n",
        "VOCAB_SIZE = tokenizer.num_words + 1   # num_words + 0:<pad>를 포함\n",
        "\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input)) # tensor --> tf.data.Dataset으로 변환\n",
        "dataset = dataset.shuffle(BUFFER_SIZE) # 완벽한 셔플링을 위해서는 데이터셋의 전체 크기보다 크거나 같은 Buffersize필요\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True) # 256개씩 묶고 나머지 제거 drop_remainder = True 요소개수가 부족한 마지막 배치 삭제\n",
        "dataset"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((256, 32), (256, 32)), types: (tf.int32, tf.int32)>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8AnomycIMjl"
      },
      "source": [
        "## Train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHFrTr7Z3aWQ"
      },
      "source": [
        "###################\n",
        "### Train model ###\n",
        "###################\n",
        "\"\"\"\n",
        "자연어 처리에서 특징 추출을 통해 수치화를 해줘야 하는데\n",
        "이때 사용하는 것이 \"언어의 벡터화\"이다.\n",
        "이런 벡터화의 과정을 Word Embedding\n",
        "\n",
        "\n",
        "RNN의 일종인 Long Short-Term Memory models(LSTM)\n",
        " Long Short Term Memory의 줄임말로 주로 시계열 처리나 자연어 처리(현재는 잘 사용 안 하지만)를 사용하는 데 사용한다\n",
        "\"\"\"\n",
        "class TextGenerator(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_size, hidden_size): \n",
        "        super().__init__()\n",
        "        # embedding Layer\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size) \n",
        "        # 인덱스 값을 해당 인덱스 번째의 워드벡터로 바꿔준다. embedding_size : 단어가 추상적으로 표현되는 크기\n",
        "        # 2개의 LSTM Layer\n",
        "        # #return_sequence:불리언. 아웃풋 시퀀스의 마지막 아웃풋을 반환할지, 혹은 시퀀스 전체를 반환할지 여부.\n",
        "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
        "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
        "\n",
        "        # 1개의 Dense Layer\n",
        "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
        "        \n",
        "    def call(self, x):\n",
        "        out = self.embedding(x)\n",
        "        out = self.rnn_1(out)\n",
        "        out = self.rnn_2(out)\n",
        "        out = self.linear(out)\n",
        "        \n",
        "        return out\n",
        "    \n",
        "embedding_size = 256\n",
        "hidden_size = 1024\n",
        "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1i_u6I_k3rmn",
        "outputId": "c58b753c-bc88-492a-e06d-ccad7d9a6efa"
      },
      "source": [
        "#모델의 input_size 설정을 위한 데이터 일부분 입력\n",
        "for src_sample, tgt_sample in dataset.take(1): break   #dataset.take(n) n번 불러옴\n",
        "model(src_sample)   #모델에 소스 데이터를 넣어준다."
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(256, 32, 12001), dtype=float32, numpy=\n",
              "array([[[-1.14694973e-04, -1.58335053e-04, -2.34046063e-04, ...,\n",
              "         -1.45526850e-04, -1.23740356e-05,  8.73077806e-05],\n",
              "        [ 1.45598708e-04, -2.77503947e-04, -3.33121599e-04, ...,\n",
              "         -4.43905214e-04, -1.25581108e-04,  2.24001531e-04],\n",
              "        [ 2.05124961e-04, -4.47647530e-04, -1.98332520e-04, ...,\n",
              "         -6.65084983e-04,  2.33842802e-05,  1.77574897e-04],\n",
              "        ...,\n",
              "        [ 1.11394773e-04, -1.91318046e-03,  2.41728779e-03, ...,\n",
              "         -5.67316497e-03, -1.18044112e-03,  1.71276450e-04],\n",
              "        [ 1.51501445e-04, -1.92936638e-03,  2.43444368e-03, ...,\n",
              "         -5.70128625e-03, -1.18233426e-03,  1.81382158e-04],\n",
              "        [ 1.87054553e-04, -1.94308278e-03,  2.44889292e-03, ...,\n",
              "         -5.72412508e-03, -1.18478888e-03,  1.90147257e-04]],\n",
              "\n",
              "       [[-1.14694973e-04, -1.58335053e-04, -2.34046063e-04, ...,\n",
              "         -1.45526850e-04, -1.23740356e-05,  8.73077806e-05],\n",
              "        [-1.16429495e-04, -1.91800485e-04, -1.94476233e-04, ...,\n",
              "         -1.90973209e-04,  2.36514243e-04,  1.49133150e-04],\n",
              "        [-2.15242588e-04, -1.57301271e-04, -1.49102096e-04, ...,\n",
              "         -7.77087698e-05,  3.17783939e-04,  1.02483209e-04],\n",
              "        ...,\n",
              "        [ 2.26175631e-04, -1.97463878e-03,  2.45740311e-03, ...,\n",
              "         -5.70615055e-03, -1.18803675e-03,  1.87033365e-04],\n",
              "        [ 2.51872232e-04, -1.98085071e-03,  2.46931636e-03, ...,\n",
              "         -5.72529202e-03, -1.19197648e-03,  1.94896376e-04],\n",
              "        [ 2.74139893e-04, -1.98594667e-03,  2.47935671e-03, ...,\n",
              "         -5.74100297e-03, -1.19597407e-03,  2.01587594e-04]],\n",
              "\n",
              "       [[-1.14694973e-04, -1.58335053e-04, -2.34046063e-04, ...,\n",
              "         -1.45526850e-04, -1.23740356e-05,  8.73077806e-05],\n",
              "        [-3.58432415e-04, -3.42212617e-04, -4.48723877e-04, ...,\n",
              "         -2.75175087e-04, -3.31146621e-05,  1.50778229e-04],\n",
              "        [-3.71955859e-04, -6.07767841e-04, -7.43314507e-04, ...,\n",
              "         -4.66153433e-04,  1.87348473e-04,  1.63754812e-04],\n",
              "        ...,\n",
              "        [ 1.92242907e-04, -1.93972967e-03,  2.42746994e-03, ...,\n",
              "         -5.70665160e-03, -1.18885655e-03,  1.69362436e-04],\n",
              "        [ 2.22316463e-04, -1.95268972e-03,  2.44155922e-03, ...,\n",
              "         -5.72872069e-03, -1.19236205e-03,  1.81068695e-04],\n",
              "        [ 2.48701050e-04, -1.96338026e-03,  2.45367060e-03, ...,\n",
              "         -5.74647449e-03, -1.19590713e-03,  1.90940322e-04]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[-1.14694973e-04, -1.58335053e-04, -2.34046063e-04, ...,\n",
              "         -1.45526850e-04, -1.23740356e-05,  8.73077806e-05],\n",
              "        [-2.18078698e-04, -2.42518596e-04, -4.89818456e-04, ...,\n",
              "         -1.95746819e-04,  8.12451399e-05,  1.52230896e-05],\n",
              "        [-2.46481824e-04, -6.72808150e-04, -6.85271807e-04, ...,\n",
              "         -3.51512921e-04,  2.82813591e-04, -1.18334967e-04],\n",
              "        ...,\n",
              "        [ 3.21301304e-05, -1.88670191e-03,  2.38641701e-03, ...,\n",
              "         -5.58494357e-03, -1.17627485e-03,  1.49775617e-04],\n",
              "        [ 8.07192555e-05, -1.90881896e-03,  2.40982859e-03, ...,\n",
              "         -5.62463794e-03, -1.18096184e-03,  1.62809913e-04],\n",
              "        [ 1.23817284e-04, -1.92720955e-03,  2.42958847e-03, ...,\n",
              "         -5.65781724e-03, -1.18530018e-03,  1.74116605e-04]],\n",
              "\n",
              "       [[-1.14694973e-04, -1.58335053e-04, -2.34046063e-04, ...,\n",
              "         -1.45526850e-04, -1.23740356e-05,  8.73077806e-05],\n",
              "        [-1.12736336e-06, -1.29183449e-04, -1.03806618e-04, ...,\n",
              "          1.72531236e-05, -3.19586601e-04,  1.25552353e-04],\n",
              "        [-2.98997387e-04, -1.32979301e-04,  2.64015525e-05, ...,\n",
              "          5.75963786e-05, -3.59505328e-04,  2.08930971e-04],\n",
              "        ...,\n",
              "        [-2.64642022e-05, -1.86228612e-03,  2.28070933e-03, ...,\n",
              "         -5.61003247e-03, -1.16853975e-03,  1.13278380e-04],\n",
              "        [ 2.73231708e-05, -1.88601832e-03,  2.31786422e-03, ...,\n",
              "         -5.64907584e-03, -1.17626216e-03,  1.31745750e-04],\n",
              "        [ 7.55791043e-05, -1.90615666e-03,  2.34966585e-03, ...,\n",
              "         -5.68145234e-03, -1.18263951e-03,  1.47780287e-04]],\n",
              "\n",
              "       [[-1.14694973e-04, -1.58335053e-04, -2.34046063e-04, ...,\n",
              "         -1.45526850e-04, -1.23740356e-05,  8.73077806e-05],\n",
              "        [-1.95589440e-04, -2.39188244e-04, -8.93718152e-06, ...,\n",
              "         -1.87341066e-04,  2.10202721e-04,  9.28056033e-05],\n",
              "        [-4.82820498e-04, -7.21640972e-05,  1.41883022e-04, ...,\n",
              "         -1.31683642e-04,  4.04244580e-04,  1.07877771e-04],\n",
              "        ...,\n",
              "        [-2.51494785e-04, -1.69628358e-03,  2.20329431e-03, ...,\n",
              "         -5.23244590e-03, -1.02001161e-03,  8.27649201e-05],\n",
              "        [-1.76774600e-04, -1.74247590e-03,  2.24891864e-03, ...,\n",
              "         -5.32397581e-03, -1.04626000e-03,  1.07774824e-04],\n",
              "        [-1.07406908e-04, -1.78285490e-03,  2.28956225e-03, ...,\n",
              "         -5.40288351e-03, -1.06797880e-03,  1.29081978e-04]]],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xNAgju107AA4",
        "outputId": "6f35c62d-997a-4243-e0cd-ec923d06d840"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"text_generator\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        multiple                  3072256   \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  multiple                  5246976   \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                multiple                  8392704   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                multiple                  12301025  \n",
            "=================================================================\n",
            "Total params: 29,012,961\n",
            "Trainable params: 29,012,961\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wumzcIxx7Bjn",
        "outputId": "e9f18926-94f3-450a-989a-ae1917b6fff6"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam() # 어떤 최적화 방법을 사용해서 loss function 값을 줄여 나갈 것인지, 여기서는 Adam 사용\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy( # 다중분류 손실함수로, 여기서 사용한 방법의 경우..\n",
        "    # sparse_categorical_crossentropy: 입력되는 출력 실측값을 그대로 사용 (지금과 같이 흔히 데이터가 각 클래스에 명확히 분류되는 경우)\n",
        "    # categorical_crossentropy: one-hot vector형태로 입력 됨 (확률적인 개념이 추가되어, 하나의 데잍터가 여러 클래스에 해단하는 경우)\n",
        "    from_logits=True, # 모델의 출력값이 확률인지(logit=False), 아닌지(logit=True)\n",
        "    reduction='none' #  모델의 출력값을 합쳐서('sum') 사용할 지 아니면 그냥 각자의 값을('none') 사용할지\n",
        ")\n",
        "\n",
        "model.compile(loss=loss, optimizer=optimizer)\n",
        "model.fit(enc_train, dec_train, validation_data=(enc_test, dec_test),epochs=10, batch_size=512)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "264/264 [==============================] - 108s 399ms/step - loss: 1.8969 - val_loss: 1.5621\n",
            "Epoch 2/10\n",
            "264/264 [==============================] - 105s 398ms/step - loss: 1.4989 - val_loss: 1.4610\n",
            "Epoch 3/10\n",
            "264/264 [==============================] - 105s 398ms/step - loss: 1.4199 - val_loss: 1.4051\n",
            "Epoch 4/10\n",
            "264/264 [==============================] - 105s 398ms/step - loss: 1.3672 - val_loss: 1.3633\n",
            "Epoch 5/10\n",
            "264/264 [==============================] - 105s 398ms/step - loss: 1.3227 - val_loss: 1.3314\n",
            "Epoch 6/10\n",
            "264/264 [==============================] - 105s 398ms/step - loss: 1.2841 - val_loss: 1.3061\n",
            "Epoch 7/10\n",
            "264/264 [==============================] - 105s 398ms/step - loss: 1.2500 - val_loss: 1.2846\n",
            "Epoch 8/10\n",
            "264/264 [==============================] - 105s 398ms/step - loss: 1.2181 - val_loss: 1.2661\n",
            "Epoch 9/10\n",
            "264/264 [==============================] - 105s 398ms/step - loss: 1.1881 - val_loss: 1.2464\n",
            "Epoch 10/10\n",
            "264/264 [==============================] - 105s 397ms/step - loss: 1.1595 - val_loss: 1.2318\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f7bc70bcd90>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIq84hjy1VeX"
      },
      "source": [
        "## Generate text & test model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvsWdVq37ION"
      },
      "source": [
        "#####################\n",
        "### Generate text ###\n",
        "#####################\n",
        "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
        "    # 테스트를 위해서 init_sentence를 텐서로 변환합니다\n",
        "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
        "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
        "    end_token = tokenizer.word_index[\"<end>\"]\n",
        "\n",
        "    while True:\n",
        "        # 1 init_sentence의 텐서를 입력합니다.\n",
        "        predict = model(test_tensor) \n",
        "        # 2 init_sentence 이후에 나올 수 있는 가장 확률 높은 단어의 word_index를 뽑아냅니다.\n",
        "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
        "        # 3 2에서 예측한 word_index를 이후에 붙입니다.\n",
        "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
        "        # 4 <end> 토크이 나오거나 max_len = 20일 경우 문장의 생성을 종료합니다.\n",
        "        if predict_word.numpy()[0] == end_token: break\n",
        "        if test_tensor.shape[1] >= max_len: break\n",
        "\n",
        "    generated = \"\"\n",
        "    for word_index in test_tensor[0].numpy():\n",
        "        generated += tokenizer.index_word[word_index] + \" \"\n",
        "\n",
        "    return generated"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "2mszR37K_U61",
        "outputId": "d8745208-b309-4ff4-84cf-6a3bcebc69c6"
      },
      "source": [
        "generate_text(model, tokenizer, init_sentence=\"<start> i love\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'<start> i love you , i m a fool <end> '"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Q5QGrkTtBnBp",
        "outputId": "e260cef3-a731-4142-ff2d-ffb82809b259"
      },
      "source": [
        "generate_text(model, tokenizer, init_sentence=\"<start> you\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'<start> you re the only one <end> '"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCfeORYa_KFU"
      },
      "source": [
        "## 결론"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6SppK2VKKM5"
      },
      "source": [
        "- 대표적인 순환 신경망 LSTM을 활용하여 특정 text를 입력받았을 때 그 뒤에 이어지는 sequential text를 예측하는 모델을 학습시킴\n",
        "- 학습이 진행될수록 train loss는 1.2###에 수렴 후 더 이상 쉽게 내려가지 않음을 확인\n",
        "- 학습된 모델은 \"i love\"를 입력받아 \"i love you , i m a fool\"의 가사를, 그리고 \"you\"를 입력받아 \"you re the only one\"라는 가사를 생성\n",
        "- 데이터 전처리 과정에서 특수문자를 제거하였으며, 토큰화 시 패딩처리 등의 과정을 수행함\n",
        "- 특히 tokenizer 생성 시 토큰 개수가 15개를 넘어가는 문장을 학습 데이터에서 제외하였으며, 데이터셋은 train:test를 8:2의 비율로 나눔\n",
        "- 총 학습 epoch는 10으로 설정하였으며, 최종 학습 loss는 1.2318가 나옴"
      ]
    }
  ]
}