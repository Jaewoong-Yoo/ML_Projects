{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KaNuzE6TOcG2"
      },
      "source": [
        "# lyricist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsD7BRV1F7Oi"
      },
      "source": [
        "## Read data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9ivxmQCaZHy",
        "outputId": "955c1f56-a75d-49ff-91e7-92eca3186bb1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "데이터 크기: 187088\n",
            "Examples:\n",
            " ['Well, summer slipped us underneath her tongue', 'Our days and nights are perfumed with obsession', 'Half of my wardrobe is on your bedroom floor']\n"
          ]
        }
      ],
      "source": [
        "import glob, os, re\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#################\n",
        "### Read data ###\n",
        "#################\n",
        "txt_file_path = r\"/content/drive/MyDrive/AIFFEL/Exploration/lyricist/data/lyrics/*\"\n",
        "\n",
        "txt_list = glob.glob(txt_file_path)\n",
        "\n",
        "raw_corpus = []\n",
        "\n",
        "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n",
        "for txt_file in txt_list:\n",
        "    with open(txt_file, \"r\") as f: # 읽기전용으로 file을 불러온다. file as f\n",
        "        raw = f.read().splitlines() # file을 한줄씩 읽어오는데 .splitlines()로 종료문자 \\n을 포함하지 않음.\n",
        "        raw_corpus.extend(raw)\n",
        "\n",
        "print(\"데이터 크기:\", len(raw_corpus))\n",
        "print(\"Examples:\\n\", raw_corpus[:3]) # 0~3 인덱스까지"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Nk6gisgF5CF"
      },
      "source": [
        "## Data cleansing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dj8oYW00z5qL",
        "outputId": "b6d4a804-d56b-48ad-973b-9b8e178e7b69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<start> this is sample sentence . <end>\n",
            "['<start> well , summer slipped us underneath her tongue <end>', '<start> our days and nights are perfumed with obsession <end>', '<start> half of my wardrobe is on your bedroom floor <end>', '<start> use our eyes , throw our hands overboard i am your sweetheart psychopathic crush <end>', '<start> drink up your movements , still i can t get enough <end>', '<start> i overthink your p punctuation use <end>', '<start> not my fault , just a thing that my mind do a rush at the beginning <end>', '<start> i get caught up , just for a minute <end>', '<start> but lover , you re the one to blame , all that you re doing <end>', '<start> can you hear the violence ? <end>']\n"
          ]
        }
      ],
      "source": [
        "################################################\n",
        "### Data cleansing using regular expressions ###\n",
        "################################################\n",
        "def preprocess_sentence(sentence):\n",
        "    sentence = sentence.lower().strip() # 소문자로 바꾸고(.lower()), 양쪽 공백 제거(.strip())\n",
        "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) # 특수문자 양쪽에 공백 삽입\n",
        "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 여러공백은 하나의 공백으로\n",
        "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) # a-zA-Z?.!,¿ 가 아닌 모든문자를 하나의 공백으로 바꾼다 // ?\n",
        "    sentence = sentence.strip() # 다시 양쪽공백지움\n",
        "    sentence = '<start> ' + sentence + ' <end>' # 문장 시작에는 <start>추가, 끝에는 <end>추가\n",
        "    return sentence\n",
        "\n",
        "print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))\n",
        "\n",
        "corpus = []   #빈 리스트 생성\n",
        "\n",
        "for sentence in raw_corpus:   #raw_corpus: 한 줄 단위로 저장된 배열\n",
        "    if len(sentence) == 0: continue   #한 글자도 없으면 continue\n",
        "    if len(sentence.split()) > 15: continue   # 토큰의 개수가 15개를 넘어가는 문장 제외\n",
        "    if sentence[-1] == \":\": continue   #마지막 글자가 \":\"이면 continue\n",
        "    \n",
        "    preprocessed_sentence = preprocess_sentence(sentence)   #정규식에 따라 문자열 변환\n",
        "    corpus.append(preprocessed_sentence)   #리스트에 추가하기\n",
        "        \n",
        "print(corpus[:10])   #0 ~ 9의 요소"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5XshBVYFz8C"
      },
      "source": [
        "## Tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ni4_dm4j1kiO",
        "outputId": "38aca2c0-871e-4622-d043-1e03aeb8adcf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor : \n",
            " [[  2 142   4 ...   0   0   0]\n",
            " [  2 153 365 ...   0   0   0]\n",
            " [  2 540  19 ...   0   0   0]\n",
            " ...\n",
            " [  2   3   0 ...   0   0   0]\n",
            " [  2   3   0 ...   0   0   0]\n",
            " [  2   3   0 ...   0   0   0]] \n",
            " tokenizer : \n",
            " <keras_preprocessing.text.Tokenizer object at 0x7f0415bb29d0>\n"
          ]
        }
      ],
      "source": [
        "################\n",
        "### Tokenize ###\n",
        "################\n",
        "def tokenize(corpus):\n",
        "    # 토큰화 시 텐서플로우의 Tokenizer와 pad_sequences 사용\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "        num_words=12000, # 12000단어를 기억할 수 있는 tokenizer를 만들기 (각 단어에 인덱스를 부여)\n",
        "        filters=' ', # filter == empty, (default값: '!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\\t\\n')\n",
        "        oov_token=\"<unk>\" # 12000단어에 포함되지 못하면 <unk>로 변환\n",
        "    )\n",
        "    tokenizer.fit_on_texts(corpus) # 문자데이터를 입력받아서 리스트의 형태로 변환\n",
        "    tensor = tokenizer.texts_to_sequences(corpus) # 텍스트 안의 단어들을 숫자의 시퀀스형태(tensor)로 변환  \n",
        "\n",
        "    # 입력 데이터의 시퀀스 길이를 일정하게 맞추되, 시퀀스가 짧으면 문장 뒤에 패딩 <pad> 붙여서 길이 맞추기\n",
        "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')  # 0을 이용하여 같은 길이의 시퀀스로 변환\n",
        "    # 만약 문장 앞에 패딩을 붙여 길이를 맞추고 싶다면 padding='pre'사용\n",
        "    \n",
        "    print('tensor :', '\\n', tensor, '\\n', 'tokenizer :', '\\n', tokenizer)\n",
        "    return tensor, tokenizer\n",
        "\n",
        "tensor, tokenizer = tokenize(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4TwPXV4w2gfw",
        "outputId": "7368c5d5-bb06-42e2-ddc5-b05fe76a6131"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[   2  142    4  557 3121  126 1217   69  957    3]\n",
            " [   2  153  365    8  833   77 9158   31 9159    3]\n",
            " [   2  540   19   13 5081   26   18   21 1454  357]]\n",
            "1 : <unk>\n",
            "2 : <start>\n",
            "3 : <end>\n",
            "4 : ,\n",
            "5 : i\n",
            "6 : the\n",
            "7 : you\n",
            "8 : and\n",
            "9 : a\n",
            "10 : to\n"
          ]
        }
      ],
      "source": [
        "print(tensor[:3, :10])\n",
        "\n",
        "for idx in tokenizer.index_word:\n",
        "    print(idx, \":\", tokenizer.index_word[idx]) # tokenizer변수에 저장되어 있는 단어 사전의 인덱스\n",
        "\n",
        "    if idx >= 10: break # word_index가 10일 때 까지 출력하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2FCC3OzFqoO"
      },
      "source": [
        "## Data splitting - train/test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "imHYIpg42q2v",
        "outputId": "91d6f91b-bfb4-484f-8f07-eff0a7f6508a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[   2  142    4  557 3121  126 1217   69  957    3    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0]\n",
            "[ 142    4  557 3121  126 1217   69  957    3    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0]\n"
          ]
        }
      ],
      "source": [
        "src_input = tensor[:, :-1]  # tensor 의 모든 행에서 마지막열을 제외한 src_input생성\n",
        "tgt_input = tensor[:, 1:] # tensor의 모든 행에서 첫번째열(start)을 제외한 tgt_input생성\n",
        "\n",
        "print(src_input[0])\n",
        "print(tgt_input[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PedDULhRGnjq",
        "outputId": "7f535a8a-e17d-4d85-8f5f-eb569178ceb0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "enc_train: (134685, 32)\n",
            "enc_test: (33672, 32)\n",
            "dec_train: (134685, 32)\n",
            "dec_test: (33672, 32)\n"
          ]
        }
      ],
      "source": [
        "# 훈련 데이터와 평가 데이터 분리, 총 데이터의 20% 를 평가 데이터셋\n",
        "enc_train, enc_test, dec_train, dec_test = train_test_split(src_input, tgt_input, test_size=0.2)\n",
        "print(f\"enc_train: {enc_train.shape}\")\n",
        "print(f\"enc_test: {enc_test.shape}\")\n",
        "print(f\"dec_train: {dec_train.shape}\")\n",
        "print(f\"dec_test: {dec_test.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9oBCfvK249S",
        "outputId": "f9d3d2a0-8c1c-4169-fb7b-e6a1bb66dba1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((256, 32), (256, 32)), types: (tf.int32, tf.int32)>"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "BUFFER_SIZE = len(src_input) #입력 문장 수\n",
        "BATCH_SIZE = 256\n",
        "steps_per_epoch = len(src_input) // BATCH_SIZE #입력 문장 수를 배치사이즈로 에포크 시행마다 나누어 훈련\n",
        "VOCAB_SIZE = tokenizer.num_words + 1   # num_words + 0:<pad>를 포함\n",
        "\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input)) # tensor --> tf.data.Dataset으로 변환\n",
        "dataset = dataset.shuffle(BUFFER_SIZE) # 완벽한 셔플링을 위해서는 데이터셋의 전체 크기보다 크거나 같은 Buffersize필요\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True) # 256개씩 묶고 나머지 제거 drop_remainder = True 요소개수가 부족한 마지막 배치 삭제\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8AnomycIMjl"
      },
      "source": [
        "## Train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "lHFrTr7Z3aWQ"
      },
      "outputs": [],
      "source": [
        "###################\n",
        "### Train model ###\n",
        "###################\n",
        "\"\"\"\n",
        "자연어 처리에서 특징 추출을 통해 수치화를 해줘야 하는데\n",
        "이때 사용하는 것이 \"언어의 벡터화\"이다.\n",
        "이런 벡터화의 과정을 Word Embedding\n",
        "\n",
        "\n",
        "RNN의 일종인 Long Short-Term Memory models(LSTM)\n",
        " Long Short Term Memory의 줄임말로 주로 시계열 처리나 자연어 처리(현재는 잘 사용 안 하지만)를 사용하는 데 사용한다\n",
        "\"\"\"\n",
        "class TextGenerator(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_size, hidden_size): \n",
        "        super().__init__()\n",
        "        # embedding Layer\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size) \n",
        "        # 인덱스 값을 해당 인덱스 번째의 워드벡터로 바꿔준다. embedding_size : 단어가 추상적으로 표현되는 크기\n",
        "        # 2개의 LSTM Layer\n",
        "        # #return_sequence:불리언. 아웃풋 시퀀스의 마지막 아웃풋을 반환할지, 혹은 시퀀스 전체를 반환할지 여부.\n",
        "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
        "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
        "\n",
        "        # 1개의 Dense Layer\n",
        "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
        "        \n",
        "    def call(self, x):\n",
        "        out = self.embedding(x)\n",
        "        out = self.rnn_1(out)\n",
        "        out = self.rnn_2(out)\n",
        "        out = self.linear(out)\n",
        "        \n",
        "        return out\n",
        "    \n",
        "embedding_size = 256\n",
        "hidden_size = 1024\n",
        "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1i_u6I_k3rmn",
        "outputId": "79f0a707-d6c3-4218-947e-067e744282b3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(256, 32, 12001), dtype=float32, numpy=\n",
              "array([[[-3.9148399e-05,  2.6511445e-05, -2.4440361e-04, ...,\n",
              "         -1.4816668e-04,  3.9087641e-05, -1.4369068e-04],\n",
              "        [ 8.2269435e-05, -1.5157358e-04, -7.2814658e-04, ...,\n",
              "         -1.6911472e-04,  1.7947330e-04, -2.3869739e-04],\n",
              "        [ 3.6555182e-05, -4.4248291e-04, -8.7222637e-04, ...,\n",
              "          9.7228120e-05,  2.7660385e-04, -2.3903971e-05],\n",
              "        ...,\n",
              "        [ 4.3841214e-03,  1.2356120e-03, -2.1948852e-04, ...,\n",
              "          2.8880730e-03, -1.1078432e-03, -9.8529586e-04],\n",
              "        [ 4.5205154e-03,  1.2410247e-03, -2.3096177e-04, ...,\n",
              "          2.9614768e-03, -1.0951747e-03, -9.8356907e-04],\n",
              "        [ 4.6391003e-03,  1.2454247e-03, -2.3644992e-04, ...,\n",
              "          3.0231229e-03, -1.0825646e-03, -9.7602094e-04]],\n",
              "\n",
              "       [[-3.9148399e-05,  2.6511445e-05, -2.4440361e-04, ...,\n",
              "         -1.4816668e-04,  3.9087641e-05, -1.4369068e-04],\n",
              "        [-2.8527016e-04,  2.7318028e-04, -4.3007804e-04, ...,\n",
              "         -2.7138286e-04,  2.0808379e-04, -2.2560728e-04],\n",
              "        [-5.8086601e-04,  5.4214720e-04, -4.4698111e-04, ...,\n",
              "         -4.5317705e-04,  4.9918424e-04, -1.3220092e-04],\n",
              "        ...,\n",
              "        [ 5.1456909e-03,  1.2753877e-03, -2.5439032e-04, ...,\n",
              "          3.2753197e-03, -1.1475853e-03, -8.6759980e-04],\n",
              "        [ 5.1745619e-03,  1.2647561e-03, -2.4099201e-04, ...,\n",
              "          3.2939003e-03, -1.1310080e-03, -8.5115782e-04],\n",
              "        [ 5.1974682e-03,  1.2540506e-03, -2.2809285e-04, ...,\n",
              "          3.3103321e-03, -1.1169191e-03, -8.3609275e-04]],\n",
              "\n",
              "       [[-3.9148399e-05,  2.6511445e-05, -2.4440361e-04, ...,\n",
              "         -1.4816668e-04,  3.9087641e-05, -1.4369068e-04],\n",
              "        [-7.4338866e-05, -8.9097892e-05, -5.7874009e-04, ...,\n",
              "         -1.7638202e-04,  1.5667669e-05, -3.3091957e-04],\n",
              "        [-3.4802657e-04,  3.5418365e-05, -4.6393013e-04, ...,\n",
              "          1.8880313e-04,  1.3865883e-04, -3.0922986e-04],\n",
              "        ...,\n",
              "        [ 5.0395313e-03,  1.2436083e-03, -4.6298874e-04, ...,\n",
              "          3.1475245e-03, -1.1960523e-03, -1.0424168e-03],\n",
              "        [ 5.0917203e-03,  1.2473213e-03, -4.1211428e-04, ...,\n",
              "          3.1907607e-03, -1.1675042e-03, -1.0041692e-03],\n",
              "        [ 5.1339311e-03,  1.2478940e-03, -3.6768866e-04, ...,\n",
              "          3.2269144e-03, -1.1429697e-03, -9.6925185e-04]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[-3.9148399e-05,  2.6511445e-05, -2.4440361e-04, ...,\n",
              "         -1.4816668e-04,  3.9087641e-05, -1.4369068e-04],\n",
              "        [-1.1744073e-04, -8.8486471e-05, -4.0153461e-04, ...,\n",
              "         -3.1869079e-04,  4.7853857e-05, -1.0355574e-04],\n",
              "        [-1.3812399e-04, -2.4194478e-05, -4.7140851e-04, ...,\n",
              "         -4.4801936e-04,  1.6973773e-04,  5.2701994e-06],\n",
              "        ...,\n",
              "        [ 5.2329279e-03,  1.2338823e-03, -1.8418090e-04, ...,\n",
              "          3.3300726e-03, -1.0842631e-03, -7.8239472e-04],\n",
              "        [ 5.2426574e-03,  1.2236834e-03, -1.7371358e-04, ...,\n",
              "          3.3431107e-03, -1.0768952e-03, -7.7695644e-04],\n",
              "        [ 5.2500349e-03,  1.2140754e-03, -1.6450512e-04, ...,\n",
              "          3.3546754e-03, -1.0708171e-03, -7.7201502e-04]],\n",
              "\n",
              "       [[-3.9148399e-05,  2.6511445e-05, -2.4440361e-04, ...,\n",
              "         -1.4816668e-04,  3.9087641e-05, -1.4369068e-04],\n",
              "        [-1.8284003e-04,  1.0059589e-04, -5.4051492e-05, ...,\n",
              "         -1.2728764e-06,  8.7030476e-06, -1.2891086e-04],\n",
              "        [-2.6469913e-04,  2.6090868e-04,  1.7755458e-04, ...,\n",
              "          5.6283217e-05, -3.5853973e-05, -1.5528633e-04],\n",
              "        ...,\n",
              "        [ 3.2535917e-03,  1.2325560e-03, -7.1987929e-04, ...,\n",
              "          2.4035834e-03, -1.8533688e-03, -1.0913832e-03],\n",
              "        [ 3.5632229e-03,  1.2510458e-03, -6.8376074e-04, ...,\n",
              "          2.5601855e-03, -1.7764195e-03, -1.0905162e-03],\n",
              "        [ 3.8324981e-03,  1.2671270e-03, -6.4386096e-04, ...,\n",
              "          2.6943299e-03, -1.6974801e-03, -1.0801931e-03]],\n",
              "\n",
              "       [[-3.9148399e-05,  2.6511445e-05, -2.4440361e-04, ...,\n",
              "         -1.4816668e-04,  3.9087641e-05, -1.4369068e-04],\n",
              "        [-1.6673407e-04,  1.7683359e-04, -3.5177753e-04, ...,\n",
              "         -1.9446939e-04,  1.7137302e-04,  4.8244303e-05],\n",
              "        [-2.6596704e-04,  3.7816429e-04, -4.7759977e-04, ...,\n",
              "         -1.7868131e-04,  6.2141288e-04,  1.5492481e-04],\n",
              "        ...,\n",
              "        [ 4.7271741e-03,  1.6204672e-03, -3.7857864e-04, ...,\n",
              "          3.0615912e-03, -1.4168172e-03, -1.0914267e-03],\n",
              "        [ 4.8406757e-03,  1.5792183e-03, -3.5283540e-04, ...,\n",
              "          3.1127741e-03, -1.3638368e-03, -1.0609890e-03],\n",
              "        [ 4.9336082e-03,  1.5403554e-03, -3.2850198e-04, ...,\n",
              "          3.1558671e-03, -1.3168075e-03, -1.0299435e-03]]], dtype=float32)>"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#모델의 input_size 설정을 위한 데이터 일부분 입력\n",
        "for src_sample, tgt_sample in dataset.take(1): break   #dataset.take(n) n번 불러옴\n",
        "model(src_sample)   #모델에 소스 데이터를 넣어준다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xNAgju107AA4",
        "outputId": "845392d6-d3dd-4104-e45c-9c74f6ec480a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"text_generator\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        multiple                  3072256   \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  multiple                  5246976   \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                multiple                  8392704   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                multiple                  12301025  \n",
            "=================================================================\n",
            "Total params: 29,012,961\n",
            "Trainable params: 29,012,961\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wumzcIxx7Bjn",
        "outputId": "e705afb7-147a-419a-de23-e3654b514dba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "657/657 [==============================] - 128s 191ms/step - loss: 1.6551\n",
            "Epoch 2/10\n",
            "657/657 [==============================] - 126s 191ms/step - loss: 1.3892\n",
            "Epoch 3/10\n",
            "657/657 [==============================] - 126s 191ms/step - loss: 1.3116\n",
            "Epoch 4/10\n",
            "657/657 [==============================] - 126s 191ms/step - loss: 1.2499\n",
            "Epoch 5/10\n",
            "657/657 [==============================] - 126s 191ms/step - loss: 1.1962\n",
            "Epoch 6/10\n",
            "657/657 [==============================] - 126s 191ms/step - loss: 1.1475\n",
            "Epoch 7/10\n",
            "657/657 [==============================] - 126s 191ms/step - loss: 1.1029\n",
            "Epoch 8/10\n",
            "657/657 [==============================] - 126s 192ms/step - loss: 1.0620\n",
            "Epoch 9/10\n",
            "657/657 [==============================] - 126s 192ms/step - loss: 1.0240\n",
            "Epoch 10/10\n",
            "657/657 [==============================] - 126s 192ms/step - loss: 0.9882\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0415c1fa10>"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "optimizer = tf.keras.optimizers.Adam() # 어떤 최적화 방법을 사용해서 loss function 값을 줄여 나갈 것인지, 여기서는 Adam 사용\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy( # 다중분류 손실함수로, 여기서 사용한 방법의 경우..\n",
        "    # sparse_categorical_crossentropy: 입력되는 출력 실측값을 그대로 사용 (지금과 같이 흔히 데이터가 각 클래스에 명확히 분류되는 경우)\n",
        "    # categorical_crossentropy: one-hot vector형태로 입력 됨 (확률적인 개념이 추가되어, 하나의 데잍터가 여러 클래스에 해단하는 경우)\n",
        "    from_logits=True, # 모델의 출력값이 확률인지(logit=False), 아닌지(logit=True)\n",
        "    reduction='none' #  모델의 출력값을 합쳐서('sum') 사용할 지 아니면 그냥 각자의 값을('none') 사용할지\n",
        ")\n",
        "\n",
        "model.compile(loss=loss, optimizer=optimizer)\n",
        "model.fit(dataset, epochs=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate text & test model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "cvsWdVq37ION"
      },
      "outputs": [],
      "source": [
        "#####################\n",
        "### Generate text ###\n",
        "#####################\n",
        "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
        "    # 테스트를 위해서 init_sentence를 텐서로 변환합니다\n",
        "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
        "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
        "    end_token = tokenizer.word_index[\"<end>\"]\n",
        "\n",
        "    while True:\n",
        "        # 1 init_sentence의 텐서를 입력합니다.\n",
        "        predict = model(test_tensor) \n",
        "        # 2 init_sentence 이후에 나올 수 있는 가장 확률 높은 단어의 word_index를 뽑아냅니다.\n",
        "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
        "        # 3 2에서 예측한 word_index를 이후에 붙입니다.\n",
        "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
        "        # 4 <end> 토크이 나오거나 max_len = 20일 경우 문장의 생성을 종료합니다.\n",
        "        if predict_word.numpy()[0] == end_token: break\n",
        "        if test_tensor.shape[1] >= max_len: break\n",
        "\n",
        "    generated = \"\"\n",
        "    for word_index in test_tensor[0].numpy():\n",
        "        generated += tokenizer.index_word[word_index] + \" \"\n",
        "\n",
        "    return generated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "2mszR37K_U61",
        "outputId": "31b9a015-3bfe-4646-a6b6-ed9829587cbe"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'<start> i love you so much <end> '"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "generate_text(model, tokenizer, init_sentence=\"<start> i love\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Q5QGrkTtBnBp",
        "outputId": "bf0d91ff-4665-494f-e0e1-2f22dd0623e2"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'<start> you re the only one who knows that <end> '"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "generate_text(model, tokenizer, init_sentence=\"<start> you\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCfeORYa_KFU"
      },
      "source": [
        "## 결론"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6SppK2VKKM5"
      },
      "source": [
        "- 대표적인 순환신경망 LSTM을 활용하여 학습된 모델은 \"i love\"를 입력받아 \"i love you so much\"의 가사를 생성함\n",
        "- 데이터 전처리 과정에서 특수문자를 제거하였으며, 토큰화 시 패딩처리 등의 과정을 수행함\n",
        "- 특히 tokenizer 생성 시 토큰 개수가 15개를 넘어가는 문장을 학습 데이터에서 제외하였으며, 데이터셋은 train:test를 8:2의 비율로 나눔\n",
        "- 총 학습 epoch는 10으로 설정하였으며, 최종 학습 loss는 0.9882가 나옴"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "lyricist.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
